{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset_precompute import PrecomputedDataset\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model arguments\n",
    "\n",
    "n_layer = 6 \n",
    "n_head = 6\n",
    "n_embd = 384\n",
    "block_size = 1024\n",
    "bias = False\n",
    "dropout = 0.0\n",
    "\n",
    "device = 'cuda'\n",
    "dtype = 'float132' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_dataloader = DataLoader(PrecomputedDataset('data/nesymres/train_nc.pt'), batch_size=64, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(PrecomputedDataset('data/nesymres/val_nc.pt'), batch_size=64, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size, bias=bias, vocab_size=14, dropout=dropout)\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf).to(device)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Accuracy and loss lists\n",
    "train_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "train_loss_list = []\n",
    "test_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device, dtype=torch.float32, non_blocking=True)\n",
    "        y = y.to(device, dtype=torch.float32, non_blocking=True)\n",
    "\n",
    "        logits, loss = model(X, y)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device, dtype=torch.float32, non_blocking=True)\n",
    "            y = y.to(device, dtype=torch.float32, non_blocking=True)\n",
    "\n",
    "            logits, loss = model(X, y)\n",
    "            pred = (torch.sigmoid(logits) > 0.5).type(torch.float16)\n",
    "\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            # multihot prediction in pred, shape is (batch_size, 14)\n",
    "            # multihot ground truth in y, shape is (batch_size, 14)\n",
    "            # correct only if all is correct\n",
    "            correct += (pred == y).all(dim=1).type(torch.float16).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    if dataloader == train_dataloader:\n",
    "        print(f\"Train Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        train_accuracy_list.append(100*correct)\n",
    "        train_loss_list.append(test_loss)\n",
    "    else:\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        test_accuracy_list.append(100*correct)\n",
    "        test_loss_list.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(f\"Epoch 0\\n-------------------------------\")\n",
    "    test_loop(val_dataloader, model)\n",
    "    test_loop(train_dataloader, model)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, model, optimizer)\n",
    "        test_loop(val_dataloader, model)\n",
    "        test_loop(train_dataloader, model)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    epochs_range = range(epochs + 1)\n",
    "\n",
    "    # Training accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_accuracy_list, label='Train Accuracy', marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Training loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_loss_list, label='Train Loss', marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
